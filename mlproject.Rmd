# Machine Learning Course Project

## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

## Cleaning Data

First step before getting into the analysis was looking at the tidy training data set to clean it properly. I removed all columns with NAs and the fisrt seven unuseful columns.

```
data = read.csv("pml-training.csv",na.strings=c("NA",""))
cleanData = subset(data, select = -c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))
cleanData = cleanData[,colSums(!is.na(cleanData)) == 19622]
```

## Data Partition
Instead of applying cross validation directly to the entire data set, I splitted it into four subsets. For that purpose, I created a function with a data frame and the percentage to cut it as parameters returning a list with the training and testing sets.

```
PartitionDataSet <- function(data, p) {
  set.seed(700)
  inTrain <- createDataPartition(y=data$classe, p=p, list=FALSE)
  training <- data[inTrain,]
  testing <- data[-inTrain,]
  return(list(training=training, testing=testing))
}
```

Using the above funtion I got the subset samples:

```
dataSet1 = PartitionDataSet(trainingData,0.25)
dataSet2 = PartitionDataSet(dataSet1$testing,0.25)
dataSet3 = PartitionDataSet(dataSet2$testing,0.5)
```

## Cross Validation
The main advantage of the above PartitionDataSet function is that I can use it also to apply cross-validation. I chose 60% for training as theoretically recommended.

```
trainingModel1 = PartitionDataSet(dataSet1$training,0.6)
trainingModel2 = PartitionDataSet(dataSet2$training,0.6)
trainingModel3 = PartitionDataSet(dataSet3$training,0.6)
trainingModel4 = PartitionDataSet(dataSet3$testing,0.6)
```

## Model - Random Forest
Reading the forum discussions and other articules, Random Forest seems to be a proper learning method for this data. The following functions are used to get the Random Forest model fit and then, predicts the output. I decided to used cross validation (cv) as training control method

```
Training <- function(training) {
  set.seed(700)
  modelFit <- train(training$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=training)
  print("********** Prediction Model **********")
  print(modelFit)
  return(modelFit)
}

Predict <- function(modelFit, testing) {
  predictions <- predict(modelFit, newdata=testing)
  print("********** Confusion Matrix **********")
  print(confusionMatrix(predictions, testing$classe))
  return(predictions)
}
```

### Model Performances

The models have an accuracy about 0.95

```
********** Prediction Model **********
Random Forest 

2946 samples
  52 predictor
   5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (4 fold) 

Summary of sample sizes: 2208, 2210, 2209, 2211 

Resampling results across tuning parameters:

  mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
   2    0.944     0.930  0.0090       0.0113  
  27    0.951     0.938  0.0119       0.0151  
  52    0.941     0.926  0.0157       0.0199  

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27. 

********** Confusion Matrix **********
Confusion Matrix and Statistics

          Reference
Prediction   A   B   C   D   E
         A 556  17   0   0   0
         B   1 357   7   0   7
         C   1   3 331  11   3
         D   0   2   4 310   4
         E   0   1   0   0 346

Overall Statistics
                                          
               Accuracy : 0.9689          
                 95% CI : (0.9602, 0.9761)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9606          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9964   0.9395   0.9678   0.9657   0.9611
Specificity            0.9879   0.9905   0.9889   0.9939   0.9994
Pos Pred Value         0.9703   0.9597   0.9484   0.9688   0.9971
Neg Pred Value         0.9986   0.9855   0.9932   0.9933   0.9913
Prevalence             0.2845   0.1938   0.1744   0.1637   0.1836
Detection Rate         0.2835   0.1820   0.1688   0.1581   0.1764
Detection Prevalence   0.2922   0.1897   0.1780   0.1632   0.1770
Balanced Accuracy      0.9921   0.9650   0.9784   0.9798   0.9802
```

### Predictions

I applied the Training and Predict functions to the four datasets, giving me four differents models fit and predictions. Below is shown the output for the 20 testing cases.

```
model 1 predictions B A B A A E D B A A B C B A E E A B A B
model 2 predictions B A A A A E D D A A B C B A E E A B B B
model 3 predictions B A B A A E D D A A B C B A E E A B B B
model 4 predictions B A B A A E D B A A B C B A E E A B A B
```

### Out of Sample Error

By definition, out fo sample error is the error you get on new data sets. In this case, the error of predicting the 20 testing cases. I used the confusion matrix to look at accurary: 0.9689, 0.9408, 0.9633, 0.9673.

The out of sample errors for this predictions are: 0.0311, 0.0592,  0.0367, 0.0327 respectivelly.